{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from keras.backend.tensorflow_backend import set_session\n",
    "# config = tf.ConfigProto()\n",
    "# config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "# set_session(tf.Session(config=config))\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, GaussianNoise, Conv1D , GRU\n",
    "from tensorflow.keras.layers import Dropout, Concatenate, Flatten, Activation, TimeDistributed, Lambda\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, multiply, concatenate, Activation, Masking, Reshape\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, GlobalAveragePooling1D, RepeatVector, Dropout\n",
    "from tensorflow.python.keras.utils import plot_model\n",
    "from tensorflow.keras.losses import MSE\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def MAE(y_te, pred):\n",
    "    return np.round(mean_absolute_error(y_te, pred), decimals=4)\n",
    "    \n",
    "def RMSE(y_te, pred):\n",
    "    return np.round(mean_squared_error(y_te, pred)**(1/2), decimals=4)\n",
    "    \n",
    "def ME(y_te, pred):\n",
    "    return np.round((y_te-pred).mean(), decimals=4)\n",
    "\n",
    "def evaluate(y_te, pred):\n",
    "    print(\"mae: {:.4f}\".format(mean_absolute_error(y_te, pred)))\n",
    "    print(\"rmse: {:.4f}\".format(mean_squared_error(y_te, pred)**(1/2)))\n",
    "    print(\"me: {:.4f}\".format((y_te-pred).mean()))\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_var = ['Target_Hb']\n",
    "\n",
    "info_vars = ['Study_Subject_Number', 'Order']\n",
    "\n",
    "categoryA = [\"Hemoglobin[Whole blood]\", \"Delta_Hb\", \"RDW[Whole blood]\", \"MCV[Whole blood]\", \"MCH[Whole blood]\", \"MCHC[Whole blood]\", \"Serum Iron[Serum]\", \"Age (yrs)\", \"Sex_M\", \"Sex_F\", 'EPO_Dose']\n",
    "\n",
    "categoryB = [\"URR[Serum]\", 'Dry Weight', 'Albumin[Serum]', 'Predialysis Weight', 'Height (cm)']\n",
    "\n",
    "trn_vars = categoryA + categoryB\n",
    "\n",
    "all_vars = info_vars + trn_vars + target_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./scaler/trn_scaler_mean.pkl','rb') as f:\n",
    "    trn_scaler_mean = pickle.load(f)\n",
    "    \n",
    "with open('./scaler/hb_scaler_mean.pkl','rb') as f:\n",
    "    hb_scaler_mean = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Recurrent Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vae():\n",
    "    batch_size = 2048\n",
    "    n_epoch = 250\n",
    "    intermediate_dim = 1024\n",
    "    latent_dim = 512\n",
    "    learning_rate = 0.001\n",
    "    seed = 0\n",
    "\n",
    "    # Q(z|X) -- encoder\n",
    "    inputs = Input(shape=(seq_len, len(trn_vars)), name='input_encoder')\n",
    "    h_q = GRU(intermediate_dim, return_sequences=True, name='gru_encoder_1')(inputs)\n",
    "    h_q = GRU(intermediate_dim, return_sequences=False, name='gru_encoder_2')(h_q)\n",
    "    z_mean = Dense(latent_dim, name='mean_encoder')(h_q)\n",
    "    z_log_var = Dense(latent_dim, activation='softplus', name='std_encoder')(h_q)\n",
    "\n",
    "    def sample_z(args):\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = K.shape(z_mean)[0]\n",
    "        dim = K.int_shape(z_mean)[1]\n",
    "        epsilon = K.random_normal(shape=(batch_size, dim), mean=0., stddev=1.)\n",
    "        return z_mean + K.exp(z_log_var/2) * epsilon # mu + var * epsilon\n",
    "\n",
    "    # Sample z ~ Q(z|X)\n",
    "    z = Lambda(sample_z, output_shape=(latent_dim), name='z_generator')([z_mean, z_log_var])\n",
    "\n",
    "    # P(X|z) -- decoder\n",
    "    decoder_h = GRU(intermediate_dim, return_sequences=True, name='gru_decoder_1')\n",
    "    decoder_mean = GRU(len(trn_vars), return_sequences=True, activation=None, name='output_decoder') \n",
    "\n",
    "    h_p = RepeatVector(seq_len)(z)\n",
    "    h_p = decoder_h(h_p)\n",
    "    x_decoder_mean = decoder_mean(h_p)\n",
    "\n",
    "    # Overall VAE model, for reconstruction and training\n",
    "    vae = Model(inputs, x_decoder_mean)\n",
    "\n",
    "    # encoder, from inputs to latent space\n",
    "    encoder = Model(inputs, [z_mean, z_log_var, z])\n",
    "\n",
    "    # Generator model, generate new data given latent variable z\n",
    "    d_in = Input(shape=(latent_dim,))\n",
    "    d_h = RepeatVector(seq_len)(d_in)\n",
    "    d_h = decoder_h(d_h)\n",
    "    d_out = decoder_mean(d_h)\n",
    "    decoder = Model(d_in, d_out)\n",
    "\n",
    "    def vae_loss(x, x_decoder_mean):\n",
    "        recon = MSE(x, x_decoder_mean)\n",
    "        kl = - 0.5 * K.mean(1 + z_log_var - K.exp(z_log_var) - K.square(z_mean))\n",
    "        return recon + kl\n",
    "\n",
    "    vae.compile(optimizer='adam', loss=vae_loss)\n",
    "    \n",
    "    return vae, encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in [2,3,4,5,6,7]:\n",
    "#     seq_len = s\n",
    "#     with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "#         loaded_data = pickle.load(f)\n",
    "#     [[x_train,y_train,train_info],[x_valid,y_valid,valid_info],[x_test,y_test,test_info]] = loaded_data\n",
    "\n",
    "#     vae, encoder, decoder = get_vae()\n",
    "\n",
    "#     checkpoint = ModelCheckpoint('./vae/vae_v1_{}.h5'.format(seq_len), save_best_only=True, verbose=0)\n",
    "\n",
    "#     # plot_model(vae, show_shapes=True)\n",
    "\n",
    "#     hist = vae.fit(x_train, x_train,\n",
    "#                     callbacks=[checkpoint],\n",
    "#                     validation_data=(x_valid, x_valid),\n",
    "#                     batch_size=batch_size,\n",
    "#                     epochs=n_epoch, verbose=0)\n",
    "\n",
    "#     plt.plot(hist.history[\"loss\"])\n",
    "#     plt.plot(hist.history[\"val_loss\"])\n",
    "#     plt.show()\n",
    "\n",
    "#     vae.load_weights('./vae/vae_v1_{}.h5'.format(seq_len))\n",
    "\n",
    "#     print('Seq', seq_len)\n",
    "#     print('train:', MAE(np.reshape(x_train, (x_train.shape[0], -1)), np.reshape(vae.predict(x_train), (x_train.shape[0], -1))))\n",
    "#     print('valid:', MAE(np.reshape(x_valid, (x_valid.shape[0], -1)), np.reshape(vae.predict(x_valid), (x_valid.shape[0], -1))))\n",
    "#     print('test:', MAE(np.reshape(x_test, (x_test.shape[0], -1)), np.reshape(vae.predict(x_test), (x_test.shape[0], -1))))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq 7\n",
      "train: 0.3085\n",
      "valid: 0.34\n",
      "test: 0.345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "seq_len = 7\n",
    "with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "[[x_train,y_train,train_info],[x_valid,y_valid,valid_info],[x_test,y_test,test_info]] = loaded_data\n",
    "\n",
    "vae, encoder, decoder = get_vae()\n",
    "\n",
    "checkpoint = ModelCheckpoint('./vae/vae_v1_{}.h5'.format(seq_len), save_best_only=True, verbose=0)\n",
    "\n",
    "# plot_model(vae, show_shapes=True)\n",
    "\n",
    "hist = vae.fit(x_train, x_train,\n",
    "               callbacks=[checkpoint],\n",
    "               validation_data=(x_valid, x_valid),\n",
    "               batch_size=batch_size,\n",
    "               epochs=n_epoch, verbose=0)\n",
    "\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.show()\n",
    "\n",
    "vae.load_weights('./vae/vae_v1_{}.h5'.format(seq_len))\n",
    "\n",
    "print('Seq', seq_len)\n",
    "print('train:', MAE(np.reshape(x_train, (x_train.shape[0], -1)), np.reshape(vae.predict(x_train), (x_train.shape[0], -1))))\n",
    "print('valid:', MAE(np.reshape(x_valid, (x_valid.shape[0], -1)), np.reshape(vae.predict(x_valid), (x_valid.shape[0], -1))))\n",
    "print('test:', MAE(np.reshape(x_test, (x_test.shape[0], -1)), np.reshape(vae.predict(x_test), (x_test.shape[0], -1))))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train: 0.3019\n",
    "# valid: 0.3445\n",
    "# test: 0.3416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean_test, z_std_test, z_test = encoder.predict(x_test, batch_size=batch_size)\n",
    "# plt.figure(figsize=(6, 6))\n",
    "# plt.scatter(z_test[:, 0], z_test[:, 1], alpha=.4, s=3**2, cmap='viridis')\n",
    "# plt.colorbar()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1353, 512)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_mean_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_batch_size = 5000\n",
    "latnet_z = np.random.normal(size=(desired_batch_size, latent_dim))\n",
    "generated_patient_data = decoder.predict(latnet_z)\n",
    "generated_x = generated_patient_data[:,:6,:]\n",
    "generated_y = np.reshape(generated_patient_data[:,-1,0], (-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in [6]:\n",
    "    seq_len = s\n",
    "    \n",
    "    with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    [[x_train,y_train,_],[x_valid,y_valid,_],[x_test,y_test,_]] = loaded_data\n",
    "    \n",
    "    X = np.concatenate([x_train, x_valid, generated_x])\n",
    "    Y = np.concatenate([y_train, y_valid, generated_y])\n",
    "    \n",
    "    models = []\n",
    "    \n",
    "    mae_train = []\n",
    "    rmse_train = []\n",
    "    me_train = []\n",
    "    \n",
    "    mae_valid = []\n",
    "    rmse_valid = []\n",
    "    me_valid = []    \n",
    "    kf = KFold(n_splits=5, random_state=seed)\n",
    "    for train, valid in kf.split(X):\n",
    "\n",
    "        x_train_data = np.reshape(X[train],[-1,seq_len*len(trn_vars)])\n",
    "        y_train_data = Y[train]\n",
    "               \n",
    "        x_valid_data = np.reshape(X[valid],[-1,seq_len*len(trn_vars)])\n",
    "        y_valid_data = Y[valid]\n",
    "\n",
    "        clf = linear_model.LinearRegression()\n",
    "        clf.fit(x_train_data, y_train_data)\n",
    "        \n",
    "        models.append(clf)        \n",
    "        \n",
    "        y_train_pred = hb_scaler_mean.inverse_transform(clf.predict(x_train_data))\n",
    "        y_valid_pred = hb_scaler_mean.inverse_transform(clf.predict(x_valid_data))\n",
    "        y_train_data = hb_scaler_mean.inverse_transform(y_train_data)\n",
    "        y_valid_data = hb_scaler_mean.inverse_transform(y_valid_data)        \n",
    "        \n",
    "        mae_train.append(MAE(y_train_data, y_train_pred))\n",
    "        rmse_train.append(RMSE(y_train_data, y_train_pred))\n",
    "        me_train.append(ME(y_train_data, y_train_pred))\n",
    "\n",
    "        mae_valid.append(MAE(y_valid_data, y_valid_pred))\n",
    "        rmse_valid.append(RMSE(y_valid_data, y_valid_pred))\n",
    "        me_valid.append(ME(y_valid_data, y_valid_pred))\n",
    "    \n",
    "    #######################################################\n",
    "    \n",
    "    print('seq : {}'.format(seq_len))\n",
    "    \n",
    "    print('train')\n",
    "    print('mae:', np.mean(mae_train, axis=0).round(4))\n",
    "    print('rmse:', np.mean(rmse_train, axis=0).round(4))\n",
    "    print('me:', np.mean(me_train, axis=0).round(4))\n",
    "\n",
    "    print('valid')\n",
    "    print('mae:', np.mean(mae_valid, axis=0).round(4))\n",
    "    print('rmse:', np.mean(rmse_valid, axis=0).round(4))\n",
    "    print('me:', np.mean(me_valid, axis=0).round(4))\n",
    "\n",
    "    print('test')\n",
    "    x_test = np.reshape(x_test,[-1,seq_len*len(trn_vars)])\n",
    "    test_preds = []\n",
    "    for model in models:\n",
    "        test_preds.append(model.predict(x_test))\n",
    "    y_test = hb_scaler_mean.inverse_transform(y_test)\n",
    "    pred_test = hb_scaler_mean.inverse_transform(np.mean(test_preds, axis=0))\n",
    "    evaluate(y_test, pred_test)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq : 6\n",
    "# train\n",
    "# mae: 0.5013\n",
    "# rmse: 0.6646\n",
    "# me: 0.0\n",
    "\n",
    "# valid\n",
    "# mae: 0.5087\n",
    "# rmse: 0.6596\n",
    "# me: 0.0017\n",
    "\n",
    "# test\n",
    "# mae: 0.6207\n",
    "# rmse: 0.8071\n",
    "# me: 0.0096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_model(i,j):\n",
    "    K.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    inp = Input(shape=(seq_len*len(trn_vars),))\n",
    "    fc = Dense(i, activation='relu')(inp)\n",
    "    fc = Dense(j, activation='relu')(fc)\n",
    "    outp = Dense(1)(fc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='mean_absolute_error', \n",
    "              optimizer=Adam(learning_rate),\n",
    "              metrics=['mean_absolute_error'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 2048\n",
    "epoch = 50\n",
    "\n",
    "for s in [6]:\n",
    "    seq_len = s\n",
    "    \n",
    "    with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    [[x_train,y_train,_],[x_valid,y_valid,_],[x_test,y_test,_]] = loaded_data\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(np.concatenate([x_train, x_valid, generated_x]), \n",
    "                                                          np.concatenate([y_train, y_valid, generated_y]),\n",
    "                                                          test_size=0.2, random_state=seed)\n",
    "                                                          \n",
    "    models = []\n",
    "    param = []\n",
    "    \n",
    "    mae_train = []\n",
    "    rmse_train = []\n",
    "    me_train = []\n",
    "    \n",
    "    mae_valid = []\n",
    "    rmse_valid = []\n",
    "    me_valid = []   \n",
    "       \n",
    "    for i in np.array([2])**list(range(1,11)):\n",
    "        print('first hidden:', i)\n",
    "        for j in np.array([2])**list(range(1,11)):\n",
    "            print('second hidden:', j)\n",
    "            param.append((i,j))\n",
    "\n",
    "            x_train_data = np.reshape(x_train,[-1,seq_len*len(trn_vars)])\n",
    "            y_train_data = y_train\n",
    "\n",
    "            x_valid_data = np.reshape(x_valid,[-1,seq_len*len(trn_vars)])\n",
    "            y_valid_data = y_valid\n",
    "\n",
    "            model = mlp_model(i,j)\n",
    "\n",
    "            checkpoint = ModelCheckpoint('./temp_models/mean_mlp_'+'_'+str(i)+'_'+str(j)+'_{}.h5'.format(seq_len), save_best_only=True, verbose=0)\n",
    "\n",
    "            hist = model.fit(x_train_data, y_train_data,\n",
    "                             callbacks=[checkpoint],\n",
    "                             validation_data=(x_valid_data, y_valid_data),\n",
    "                             epochs=epoch, verbose=0, batch_size=batch_size)\n",
    "\n",
    "\n",
    "            y_train_pred = hb_scaler_mean.inverse_transform(model.predict(x_train_data))\n",
    "            y_valid_pred = hb_scaler_mean.inverse_transform(model.predict(x_valid_data))\n",
    "            y_train_data = hb_scaler_mean.inverse_transform(y_train_data)\n",
    "            y_valid_data = hb_scaler_mean.inverse_transform(y_valid_data)                \n",
    "\n",
    "            mae_train.append(MAE(y_train_data, y_train_pred))\n",
    "            rmse_train.append(RMSE(y_train_data, y_train_pred))\n",
    "            me_train.append(ME(y_train_data, y_train_pred))\n",
    "\n",
    "            mae_valid.append(MAE(y_valid_data, y_valid_pred))\n",
    "            rmse_valid.append(RMSE(y_valid_data, y_valid_pred))\n",
    "            me_valid.append(ME(y_valid_data, y_valid_pred))                \n",
    "                \n",
    "            models.append(model)\n",
    "    \n",
    "    ###############################################################################\n",
    "    \n",
    "    print('seq : {}'.format(seq_len))\n",
    "    best_param_idx = np.argmin(mae_valid)\n",
    "    i, j = param[best_param_idx]\n",
    "    print('best result at:', param[best_param_idx])\n",
    "    \n",
    "    print('train')\n",
    "    print('mae:', mae_train[best_param_idx])\n",
    "    print('rmse:', rmse_train[best_param_idx])\n",
    "    print('me:', me_train[best_param_idx])\n",
    "\n",
    "    print('valid')\n",
    "    print('mae:', mae_valid[best_param_idx])\n",
    "    print('rmse:', rmse_valid[best_param_idx])\n",
    "    print('me:', me_valid[best_param_idx])\n",
    "\n",
    "    print('test')\n",
    "    x_test = np.reshape(x_test,[-1,seq_len*len(trn_vars)])\n",
    "    test_preds = []\n",
    "    model = models[best_param_idx]\n",
    "    model.load_weights('./temp_models/mean_mlp_'+'_'+str(i)+'_'+str(j)+'_{}.h5'.format(seq_len))\n",
    "    y_test = hb_scaler_mean.inverse_transform(y_test)\n",
    "    pred_test = hb_scaler_mean.inverse_transform(model.predict(x_test))\n",
    "    evaluate(y_test, pred_test)     \n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 300\n",
    "batch_size = 2048\n",
    "seed = 0\n",
    "sequences = [2,3,4,5,6]\n",
    "\n",
    "def get_model():\n",
    "    K.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    inp = Input(shape=(seq_len, len(trn_vars)))\n",
    "    layer1 = GRU(256, return_sequences=True,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(inp)\n",
    "    layer2 = GRU(64, return_sequences=False,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(layer1)\n",
    "    fc = Dense(4)(layer2)\n",
    "    outp = Dense(1)(fc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss = 'mean_absolute_error', \n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "for s in tqdm(sequences) :\n",
    "    seq_len = s\n",
    "\n",
    "    with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    [[x_train,y_train,_],[x_valid,y_valid,_],[x_test,y_test,_]] = loaded_data\n",
    "    \n",
    "    ##\n",
    "#     vae, encoder, decoder = get_vae()\n",
    "#     vae.load_weights('./vae/vae_v1_{}.h5'.format(seq_len+1))\n",
    "\n",
    "#     desired_batch_size = 10000\n",
    "#     latnet_z = np.random.normal(size=(desired_batch_size, latent_dim))\n",
    "#     generated_patient_data = decoder.predict(latnet_z)\n",
    "#     generated_x = generated_patient_data[:,:seq_len,:]\n",
    "#     generated_y = np.reshape(generated_patient_data[:,-1,0], (-1,1))\n",
    "\n",
    "#     x_train = np.concatenate([x_train, generated_x])\n",
    "#     y_train = np.concatenate([y_train, generated_y])\n",
    "    ##\n",
    "    \n",
    "#     x_train, x_valid, y_train, y_valid = train_test_split(np.concatenate([x_train, x_valid, generated_x]), \n",
    "#                                                           np.concatenate([y_train, y_valid, generated_y]),\n",
    "#                                                           test_size=0.2, random_state=seed)\n",
    "\n",
    "    model = get_model()\n",
    "\n",
    "    checkpoint = ModelCheckpoint('./temp_models/gru_mean_wo_gauss{}.h5'.format(seq_len), save_best_only=True, verbose=0)\n",
    "\n",
    "    hist = model.fit(np.array(x_train), np.array(y_train), \n",
    "                     callbacks=[checkpoint],\n",
    "                     validation_data=(np.array(x_valid), np.array(y_valid)), \n",
    "                     batch_size=batch_size, epochs=epochs, verbose=0) \n",
    "\n",
    "    plt.plot(hist.history[\"loss\"])\n",
    "    plt.plot(hist.history[\"val_loss\"])\n",
    "    plt.show()\n",
    "\n",
    "    model.load_weights('./temp_models/gru_mean_wo_gauss{}.h5'.format(seq_len))\n",
    "\n",
    "    print(seq_len)\n",
    "    pred_train = model.predict(x_train)\n",
    "    print('train')\n",
    "    y_train = hb_scaler_mean.inverse_transform(y_train)\n",
    "    pred_train = hb_scaler_mean.inverse_transform(pred_train)\n",
    "    evaluate(y_train, pred_train)\n",
    "\n",
    "    pred_valid = model.predict(x_valid)\n",
    "    print('val')\n",
    "    y_valid = hb_scaler_mean.inverse_transform(y_valid)\n",
    "    pred_valid = hb_scaler_mean.inverse_transform(pred_valid)\n",
    "    evaluate(y_valid, pred_valid)\n",
    "\n",
    "    pred_test = model.predict(x_test)\n",
    "    print('test')\n",
    "    y_test = hb_scaler_mean.inverse_transform(y_test)\n",
    "    pred_test = hb_scaler_mean.inverse_transform(pred_test)\n",
    "    evaluate(y_test, pred_test)\n",
    "\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "train\n",
      "mae: 0.3838\n",
      "rmse: 0.5956\n",
      "me: -0.0006\n",
      "val\n",
      "mae: 0.6086\n",
      "rmse: 0.7883\n",
      "me: 0.0118\n",
      "test\n",
      "mae: 0.6099\n",
      "rmse: 0.7968\n",
      "me: -0.0227\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 2048\n",
    "seed = 0\n",
    "sequences = [2,3,4,5,6]\n",
    "\n",
    "def get_model():\n",
    "    K.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    inp = Input(shape=(seq_len, len(trn_vars)))\n",
    "    layer1 = GRU(256, return_sequences=True,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(inp)\n",
    "    layer2 = GRU(64, return_sequences=False,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(layer1)\n",
    "    fc = Dense(4)(layer2)\n",
    "    outp = Dense(1)(fc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss = 'mean_absolute_error', \n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "seq_len = 6\n",
    "\n",
    "with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "[[x_train,y_train,_],[x_valid,y_valid,_],[x_test,y_test,_]] = loaded_data\n",
    "\n",
    "vae, encoder, decoder = get_vae()\n",
    "vae.load_weights('./vae/vae_v1_{}.h5'.format(seq_len+1))\n",
    "desired_batch_size = 5000\n",
    "latnet_z = np.random.normal(size=(desired_batch_size, latent_dim))\n",
    "generated_patient_data = decoder.predict(latnet_z)\n",
    "generated_x = generated_patient_data[:,:seq_len,:]\n",
    "generated_y = np.reshape(generated_patient_data[:,-1,0], (-1,1))\n",
    "\n",
    "x_train = np.concatenate([x_train, generated_x])\n",
    "y_train = np.concatenate([y_train, generated_y])\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "checkpoint = ModelCheckpoint('./temp_models/gru_mean_wo_gauss{}.h5'.format(seq_len), save_best_only=True, verbose=0)\n",
    "\n",
    "hist = model.fit(x_train, y_train, \n",
    "                 callbacks=[checkpoint],\n",
    "                 validation_data=(np.array(x_valid), np.array(y_valid)), \n",
    "                 batch_size=batch_size, epochs=epochs, verbose=0) \n",
    "\n",
    "plt.plot(hist.history[\"loss\"])\n",
    "plt.plot(hist.history[\"val_loss\"])\n",
    "plt.show()\n",
    "\n",
    "model.load_weights('./temp_models/gru_mean_wo_gauss{}.h5'.format(seq_len))\n",
    "\n",
    "print(seq_len)\n",
    "pred_train = model.predict(x_train)\n",
    "print('train')\n",
    "y_train = hb_scaler_mean.inverse_transform(y_train)\n",
    "pred_train = hb_scaler_mean.inverse_transform(pred_train)\n",
    "evaluate(y_train, pred_train)\n",
    "\n",
    "pred_valid = model.predict(x_valid)\n",
    "print('val')\n",
    "y_valid = hb_scaler_mean.inverse_transform(y_valid)\n",
    "pred_valid = hb_scaler_mean.inverse_transform(pred_valid)\n",
    "evaluate(y_valid, pred_valid)\n",
    "\n",
    "pred_test = model.predict(x_test)\n",
    "print('test')\n",
    "y_test = hb_scaler_mean.inverse_transform(y_test)\n",
    "pred_test = hb_scaler_mean.inverse_transform(pred_test)\n",
    "evaluate(y_test, pred_test)\n",
    "\n",
    "print()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# train\n",
    "# mae: 0.4250\n",
    "# rmse: 0.5899\n",
    "# me: -0.0096\n",
    "# val\n",
    "# mae: 0.5846\n",
    "# rmse: 0.7675\n",
    "# me: -0.0162\n",
    "# test\n",
    "# mae: 0.6086\n",
    "# rmse: 0.7936\n",
    "# me: 0.0034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU with Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 2048 \n",
    "seed = 0\n",
    "sequences = [2,3,4,5,6]\n",
    "\n",
    "def get_model():        \n",
    "    K.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    inp = Input(shape=(seq_len, len(trn_vars)))\n",
    "    gauss1 = GaussianNoise(0.1)(inp)\n",
    "    layer1 = GRU(256, return_sequences=True,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(gauss1)\n",
    "    layer2 = GRU(64, return_sequences=False,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(layer1)   \n",
    "    fc = Dense(4)(layer2)\n",
    "    outp = Dense(1)(fc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss = 'mean_absolute_error', \n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "\n",
    "for s in tqdm([6]) :\n",
    "    seq_len = s\n",
    "    \n",
    "    with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    [[x_train,y_train,_],[x_valid,y_valid,_],[x_test,y_test,_]] = loaded_data\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(np.concatenate([x_train, x_valid, generated_x]), \n",
    "                                                          np.concatenate([y_train, y_valid, generated_y]),\n",
    "                                                          test_size=0.2, random_state=seed)\n",
    "    \n",
    "    model = get_model()\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('./temp_models/gru_mean_{}.h5'.format(seq_len), save_best_only=True, verbose=0)\n",
    "\n",
    "    hist = model.fit(np.array(x_train), np.array(y_train), \n",
    "                     callbacks=[checkpoint],\n",
    "                     validation_data=(np.array(x_valid), np.array(y_valid)), \n",
    "                     batch_size=batch_size, epochs=epochs, verbose=0) \n",
    "\n",
    "    plt.plot(hist.history[\"loss\"])\n",
    "    plt.plot(hist.history[\"val_loss\"])\n",
    "    plt.show()\n",
    "\n",
    "    model.load_weights('./temp_models/gru_mean_{}.h5'.format(seq_len))\n",
    "    \n",
    "    print(seq_len)\n",
    "    pred_train = model.predict(x_train)\n",
    "    print('train')\n",
    "    y_train = hb_scaler_mean.inverse_transform(y_train)\n",
    "    pred_train = hb_scaler_mean.inverse_transform(pred_train)\n",
    "    evaluate(y_train, pred_train)\n",
    "\n",
    "    pred_valid = model.predict(x_valid)\n",
    "    print('val')\n",
    "    y_valid = hb_scaler_mean.inverse_transform(y_valid)\n",
    "    pred_valid = hb_scaler_mean.inverse_transform(pred_valid)\n",
    "    evaluate(y_valid, pred_valid)\n",
    "\n",
    "    pred_test = model.predict(x_test)\n",
    "    print('test')\n",
    "    y_test = hb_scaler_mean.inverse_transform(y_test)\n",
    "    pred_test = hb_scaler_mean.inverse_transform(pred_test)\n",
    "    evaluate(y_test, pred_test)\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance by Hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 6\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "batch_size = 2048 \n",
    "seed = 0\n",
    "\n",
    "def get_model():        \n",
    "    K.clear_session()\n",
    "    np.random.seed(seed)\n",
    "    inp = Input(shape=(seq_len, len(trn_vars)))\n",
    "    gauss1 = GaussianNoise(0.1)(inp)\n",
    "    layer1 = GRU(256, return_sequences=True,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(gauss1)\n",
    "    layer2 = GRU(64, return_sequences=False,\n",
    "                recurrent_activation='hard_sigmoid',\n",
    "                activation='tanh')(layer1)   \n",
    "    fc = Dense(4)(layer2)\n",
    "    outp = Dense(1)(fc)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss = 'mean_absolute_error', \n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.load_weights('./temp_models/gru_mean_{}.h5'.format(seq_len))\n",
    "\n",
    "with open('./data/mean_{}.pkl'.format(seq_len),'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "[[x_train,y_train,train_info],[x_valid,y_valid,valid_info],[x_test,y_test,test_info]] = loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_info = np.array([i[0] for i in train_info[:,0]])\n",
    "valid_info = np.array([i[0] for i in valid_info[:,0]])\n",
    "test_info = np.array([i[0] for i in test_info[:,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for h in ['A', 'B', 'D', 'E', 'F', 'G', 'H']:\n",
    "    print('Hospital:', h)\n",
    "    \n",
    "    pred_train = model.predict(x_train[train_info==h])\n",
    "    print('train')\n",
    "    pred_train = hb_scaler_mean.inverse_transform(pred_train)\n",
    "    print('mae:', MAE(hb_scaler_mean.inverse_transform(y_train[train_info==h]), pred_train))\n",
    "\n",
    "    pred_valid = model.predict(x_valid[valid_info==h])\n",
    "    print('val')\n",
    "    pred_valid = hb_scaler_mean.inverse_transform(pred_valid)\n",
    "    print('mae:', MAE(hb_scaler_mean.inverse_transform(y_valid[valid_info==h]), pred_valid))\n",
    "\n",
    "    pred_test = model.predict(x_test[test_info==h])\n",
    "    print('test')\n",
    "    pred_test = hb_scaler_mean.inverse_transform(pred_test)\n",
    "    print('mae:', MAE(hb_scaler_mean.inverse_transform(y_test[test_info==h]), pred_test))\n",
    "\n",
    "    print()\n",
    "    print()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
